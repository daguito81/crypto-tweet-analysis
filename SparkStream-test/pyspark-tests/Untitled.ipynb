{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StringType,\n",
    "                               IntegerType, FloatType,\n",
    "                               BooleanType, ArrayType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Test1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092, localhost:9093, localhost:9094\")\\\n",
    "    .option(\"subscribe\", \"nifi-test\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream_value = df_stream.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream_value.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stream_value.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallBatch = spark.read.format(\"kafka\")\\\n",
    "                           .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "                           .option(\"subscribe\", \"nifi-test\")\\\n",
    "                           .option(\"startingOffsets\", \"earliest\")\\\n",
    "                           .option(\"endingOffsets\", \"\"\"{\"nifi-test\":{\"0\":2}}\"\"\")\\\n",
    "                           .load()\\\n",
    "                           .selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallBatch.write.mode(\"overwrite\").format('text').save(\"/home/daguito81/tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet = spark.read.json(\"/home/daguito81/tweets/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_schema = tweet.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092, localhost:9093, localhost:9094\")\\\n",
    "    .option(\"subscribe\", \"nifi-test\")\\\n",
    "    .option(\"value.deserializer\", \"json\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_value = df_eth_batch.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_value.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_batch.select(['partition', 'offset']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_test = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092, localhost:9093, localhost:9094\")\\\n",
    "    .option(\"subscribe\", \"nifi-test\")\\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), tweet_schema).alias(\"parsed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_test.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_schema = spark.read.json(\"/home/daguito81/tweets/*\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream_eth = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092, localhost:9093, localhost:9094\")\\\n",
    "    .option(\"subscribe\", \"ethereum\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), tweet_schema).alias(\"parsed_value\"))\\\n",
    "    .selectExpr(\"parsed_value.entities.hashtags\",\n",
    "                  \"parsed_value.favorite_count\",\n",
    "                  \"parsed_value.lang\",\n",
    "                  \"parsed_value.retweet_count\",\n",
    "                  \"parsed_value.text\",\n",
    "                  \"parsed_value.user.followers_count\",\n",
    "                  \"parsed_value.user.name\",\n",
    "                  \"parsed_value.id\",\n",
    "                  \"parsed_value.created_at\")\\\n",
    "    .selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"eth-processed\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/daguito81/Desktop/crypto-tweet-analysis/SparkStream-test/pyspark-tests/checkpoints\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream_btc = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092, localhost:9093, localhost:9094\")\\\n",
    "    .option(\"subscribe\", \"bitcoin\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), tweet_schema).alias(\"parsed_value\"))\\\n",
    "    .selectExpr(\"parsed_value.entities.hashtags\",\n",
    "                  \"parsed_value.favorite_count\",\n",
    "                  \"parsed_value.lang\",\n",
    "                  \"parsed_value.retweet_count\",\n",
    "                  \"parsed_value.text\",\n",
    "                  \"parsed_value.user.followers_count\",\n",
    "                  \"parsed_value.user.name\",\n",
    "                  \"parsed_value.id\",\n",
    "                  \"parsed_value.created_at\")\\\n",
    "    .selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"btc-processed\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/daguito81/Desktop/crypto-tweet-analysis/SparkStream-test/pyspark-tests/checkpoints2\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = df_stream.select(from_json(col(\"value\").cast(\"string\"), tweet_schema).alias(\"parsed_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = new_df.selectExpr(\"parsed_value.entities.hashtags\",\n",
    "#                   \"parsed_value.favorite_count\",\n",
    "#                   \"parsed_value.lang\",\n",
    "#                   \"parsed_value.retweet_count\",\n",
    "#                   \"parsed_value.text\",\n",
    "#                   \"parsed_value.user.followers_count\",\n",
    "#                   \"parsed_value.user.name\",\n",
    "#                   \"parsed_value.id\",\n",
    "#                   \"parsed_value.created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process = final_df \\\n",
    "#   .selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "#   .writeStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"topic\", \"tweets-processed\") \\\n",
    "#   .option(\"checkpointLocation\", \"/home/daguito81/Desktop/crypto-tweet-analysis/SparkStream-test/pyspark-tests/checkpoints\") \\\n",
    "#   .trigger(continuous=\"1 second\") \\ \n",
    "#   .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kafka]",
   "language": "python",
   "name": "conda-env-kafka-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
